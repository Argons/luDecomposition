{"name":"Lu Decomposition","tagline":"LU decomposition of a large matrix using OpenMP and MPI","body":"Parallel Implementation of LU decomposition\r\n======\r\n\r\n###Basics\r\n======\r\n\r\n-   Root directory contains three sub-directories namely ’Sequential’,\r\n    ’OpenMP’ and ’MPI’.\r\n\r\n-   Each subdirectory has source code in the form of ** ’\\*.c’** file.\r\n\r\n-   Matrix is generated in a manner that it decomposes into a L and U\r\n    containing only 1s and 0s.\r\n\r\n-   To submit jobs for various configurations run **’./submit.sh’** on\r\n    terminal. This will automatically submit all the jobs in the\r\n    subdirectory to the **general-compute** queue of the ccr cluster.\r\n\r\n-   Outputs are generated in the **output.txt** file. *Sample outputs\r\n    are included*.\r\n\r\n-   In the corresponding subdirectory run **./plot.sh** on the linux\r\n    terminal to generate a graphical visualization of the output.\r\n    *gnuplot is required to generate graph*.\r\n\r\n-   Graphs are generated as **’Plot.pdf’**. *Please wait for the job run\r\n    to finish and outputs to accumulate*.\r\n\r\n###Sequential Implementation\r\n=========================\r\n\r\n-   Gaussian elimination algorithm was implemented that sequentially\r\n    decomposes the square matrix.\r\n\r\n-   Algorithm was evaluated on input matrix size of 1000, 5000, 10000,\r\n    20000.\r\n\r\n-   Time taken to decompose the matrix grew exponentially with the\r\n    increase in size.\r\n\r\n-   Since, this was a sequential implementation increase in compute\r\n    nodes won’t do anything.\r\n\r\n-   Since I was using Gaussian elimination that computes L and U\r\n    matrices separately, I ran **out of memory** when matrix size of\r\n    50,000 was tried. This implementation makes two copies of the matrix\r\n    of same size as input.\r\n\r\n![Sequential Decomposition Algorithm](http://raw.github.com/adilansari/luDecomposition/master/Sequential/Plot.png)\r\n\r\n###OpenMP Implementation\r\n=====================\r\n\r\n-   Gaussian elimination algorithm was implemented that uses the block\r\n    wise decomposition in parallel.\r\n\r\n-   The **for** loops are parallelized in a manner that blocks of\r\n    matrices are decomposed by dividing the work among parallel threads.\r\n\r\n-   Algorithm was evaluated for input matrix of sizes 1000, 5000, 10000,\r\n    20000 with a combination of 2, 4, 8, 16, 32 threads executing in\r\n    parallel.\r\n\r\n-   On a fixed workload the decompostion was faster when more threads\r\n    are executing in parallel. The execution was comparatively faster on\r\n    larger workload due to the fact, parallelism was more effective.\r\n\r\n-   For a fixed number of cores the time increased exponentially with\r\n    increase in matrix size.\r\n\r\n-   The parallelism was ineffective on relatively smaller loads.\r\n\r\n-   Since I was using Gaussian elimination that computes L and U\r\n    matrices separately, I ran **out of memory** when matrix size of\r\n    50,000 was tried. This implementation makes two copies of the matrix\r\n    of same size as input.\r\n\r\n![OpenMP Decomposition Algorithm](http://raw.github.com/adilansari/luDecomposition/master/OpenMP/Plot.png)\r\n\r\n###MPI Implementation\r\n==================\r\n\r\n-   Cyclic distribution was used to accomplish LU factorization of the\r\n    input square matrix.\r\n\r\n-   Each node is responsible for computing its own block and broadcast\r\n    the result to rest of the nodes.\r\n\r\n-   Algorithm was evaluated for input matrix of sizes 1000, 5000, 10000\r\n    with a combination of 8, 16, 32 compute nodes working in parallel.\r\n\r\n-   For a fixed number of compute nodes the algorithm showed uniform\r\n    behavior.\r\n\r\n-   For fixed workload the parallelism was more effective for larger\r\n    workloads on maximum compute nodes.\r\n\r\n\r\n![MPI Decomposition Algorithm](http://raw.github.com/adilansari/luDecomposition/master/MPI/Plot.png)\r\n\r\nComparison\r\n==========\r\n\r\n-   Since, MPI involves communication overhead between different nodes,\r\n    it was slower as compared to OpenMP.\r\n\r\n    ![OpenMP vs. MPI](http://raw.github.com/adilansari/luDecomposition/master/Graphs/omp_mpi_core_time.png)\r\n\r\n-   As expected sequential algorithm turns out to be the worst performer\r\n    of the three.\r\n\r\n    ![Sequential vs. OpenMP vs.MPI](http://raw.github.com/adilansari/luDecomposition/master/Graphs/omp_mpi_seq_siz_time.png)\r\n\r\n###Scalability\r\n===========\r\n\r\nLU factorization algorithm has a great extent of parallelization when\r\nscaled appropriately.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}